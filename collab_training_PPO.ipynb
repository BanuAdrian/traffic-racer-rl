{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bVnx2fWwqpZ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Endless Overtake Game - 9 Actions (Combined Steering + Gas/Brake)\n",
        "\"\"\"\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "import pygame\n",
        "from typing import Optional, Tuple, List\n",
        "\n",
        "# Constante\n",
        "SCREEN_WIDTH = 400\n",
        "SCREEN_HEIGHT = 900 # H = 900 makes game easier\n",
        "CAR_WIDTH = 40\n",
        "CAR_HEIGHT = 70\n",
        "LANE_WIDTH = 80\n",
        "ROAD_LEFT = 40\n",
        "\n",
        "LANE_X = [\n",
        "    ROAD_LEFT + LANE_WIDTH * 0.5,   # Lane 0 - contrasens exterior\n",
        "    ROAD_LEFT + LANE_WIDTH * 1.5,   # Lane 1 - contrasens interior\n",
        "    ROAD_LEFT + LANE_WIDTH * 2.5,   # Lane 2 - sens nostru interior\n",
        "    ROAD_LEFT + LANE_WIDTH * 3.5,   # Lane 3 - sens nostru exterior\n",
        "]\n",
        "\n",
        "BLACK = (30, 30, 30)\n",
        "WHITE = (255, 255, 255)\n",
        "GRAY = (80, 80, 80)\n",
        "YELLOW = (255, 255, 0)\n",
        "GREEN = (50, 200, 50)\n",
        "BLUE = (50, 120, 220)\n",
        "ORANGE = (255, 140, 0)\n",
        "RED = (200, 50, 50)\n",
        "\n",
        "\n",
        "class Car:\n",
        "    def __init__(self, lane: int, y: float, speed: float, color: Tuple):\n",
        "        self.lane = lane\n",
        "        self.x = LANE_X[lane]\n",
        "        self.y = y\n",
        "        self.speed = speed\n",
        "        self.color = color\n",
        "    \n",
        "    def get_rect(self) -> pygame.Rect:\n",
        "        return pygame.Rect(\n",
        "            self.x - CAR_WIDTH // 2,\n",
        "            self.y - CAR_HEIGHT // 2,\n",
        "            CAR_WIDTH,\n",
        "            CAR_HEIGHT\n",
        "        )\n",
        "\n",
        "\n",
        "class EndlessOvertakeEnv(gym.Env):\n",
        "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 30}\n",
        "    \n",
        "    def __init__(self, render_mode: Optional[str] = None):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.render_mode = render_mode\n",
        "        self.screen = None\n",
        "        self.clock = None\n",
        "        \n",
        "        # Action space EXTINS la 9 actiuni pentru control fluid:\n",
        "        # 0: Idle, 1: Accel, 2: Brake, 3: Left, 4: Right\n",
        "        # 5: Accel+Left, 6: Accel+Right, 7: Brake+Left, 8: Brake+Right\n",
        "        self.action_space = spaces.Discrete(9)\n",
        "        \n",
        "        # Observation: [ego_lane, ego_speed, x_offset, car1_lane, car1_rel_y, car1_rel_speed, ...]\n",
        "        # 3 (ego) + 10 caars * 3 = 33\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-np.inf, high=np.inf, shape=(33,), dtype=np.float32\n",
        "        )\n",
        "        \n",
        "        # Config\n",
        "        self.ego_y = SCREEN_HEIGHT * 0.7  # ego fix pe ecran\n",
        "        self.min_speed = 10\n",
        "        self.max_speed = 100\n",
        "        self.same_dir_speed_range = (12, 45)   # albastru - mai lente\n",
        "        self.oncoming_speed_range = (10, 27)   # portocaliu\n",
        "        self.spawn_gap = 120\n",
        "        self.target_cars_per_lane = 2\n",
        "        \n",
        "        # keep this from 1 - 10 i think\n",
        "        self.traffic_density = 4\n",
        "        \n",
        "        self.reset()\n",
        "    \n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        \n",
        "        # Ego vehicle\n",
        "        self.ego_lane = 2\n",
        "        self.ego_speed = 30.0\n",
        "        self.ego_x = LANE_X[self.ego_lane]\n",
        "        self.move_speed = 4\n",
        "        \n",
        "        self.x_min = ROAD_LEFT + CAR_WIDTH // 2 + 5\n",
        "        self.x_max = ROAD_LEFT + LANE_WIDTH * 4 - CAR_WIDTH // 2 - 5\n",
        "        \n",
        "        self.world_y = 0.0\n",
        "        \n",
        "        self.cars: List[Car] = []\n",
        "        \n",
        "        self.spawn_timer = 0\n",
        "        self.active_pattern = None   # 'COLUMN', 'WALL', 'SINGLE' sau None\n",
        "        self.pattern_counter = 0     # CÃ¢te maÈ™ini au mai rÄƒmas de spawnat Ã®n pattern\n",
        "        self.pattern_lane = 0        # Pe ce bandÄƒ e pattern-ul curent\n",
        "        \n",
        "        self._spawn_car_on_lane(self.np_random.integers(0, 4), -300)\n",
        "        \n",
        "        self.steps = 0\n",
        "        self.total_distance = 0\n",
        "        self.overtakes = 0\n",
        "        self.overtaken_cars = set()\n",
        "        \n",
        "        return self._get_obs(), {}\n",
        "    \n",
        "    \n",
        "    def _maintain_traffic_density(self):\n",
        "        self.spawn_timer -= 1\n",
        "        \n",
        "        if self.spawn_timer > 0:\n",
        "            return\n",
        "\n",
        "        spawn_y = -CAR_HEIGHT - 50\n",
        "\n",
        "        speed_factor = 60.0 / max(self.ego_speed, 15.0)\n",
        "\n",
        "        if self.active_pattern == 'COLUMN' and self.pattern_counter > 0:\n",
        "            if self._is_lane_safe(self.pattern_lane, spawn_y):\n",
        "                self._spawn_car_on_lane(self.pattern_lane, spawn_y)\n",
        "                self.pattern_counter -= 1\n",
        "                \n",
        "                self.spawn_timer = 15\n",
        "                \n",
        "                if self.pattern_counter == 0:\n",
        "                    self.active_pattern = None\n",
        "                    base_wait = int(70 / self.traffic_density)\n",
        "                    self.spawn_timer = int(base_wait * speed_factor)\n",
        "            else:\n",
        "                self.spawn_timer = 5\n",
        "            return\n",
        "\n",
        "        self.active_pattern = None\n",
        "        \n",
        "        roll = self.np_random.random()\n",
        "        \n",
        "        # 40% chance - SINGLE CAR\n",
        "        if roll < 0.40:\n",
        "            lane = self.np_random.integers(0, 4)\n",
        "            if self._is_lane_safe(lane, spawn_y):\n",
        "                self._spawn_car_on_lane(lane, spawn_y)\n",
        "                base_wait = int(40 / self.traffic_density)\n",
        "                self.spawn_timer = int(base_wait * speed_factor)\n",
        "            else:\n",
        "                self.spawn_timer = 5\n",
        "\n",
        "        # 30% - WALL (ZID pe 2 benzi)\n",
        "        elif roll < 0.70:\n",
        "            start_lane = self.np_random.integers(0, 3)\n",
        "            lane_a = start_lane\n",
        "            lane_b = start_lane + 1\n",
        "            \n",
        "            if self._is_lane_safe(lane_a, spawn_y) and self._is_lane_safe(lane_b, spawn_y):\n",
        "                self._spawn_car_on_lane(lane_a, spawn_y)\n",
        "                self._spawn_car_on_lane(lane_b, spawn_y)\n",
        "                \n",
        "                base_wait = int(80 / self.traffic_density)\n",
        "                self.spawn_timer = int(base_wait * speed_factor)\n",
        "            else:\n",
        "                self.spawn_timer = 5\n",
        "\n",
        "        # 30% - TRAFFIC COLUMN\n",
        "        else:\n",
        "            lane = self.np_random.integers(0, 4)\n",
        "            if self._is_lane_safe(lane, spawn_y):\n",
        "\n",
        "                self.active_pattern = 'COLUMN'\n",
        "                self.pattern_lane = lane\n",
        "                self.pattern_counter = self.np_random.integers(2, 5) # 2-4 maÈ™ini\n",
        "                \n",
        "                self._spawn_car_on_lane(lane, spawn_y)\n",
        "                self.pattern_counter -= 1\n",
        "                self.spawn_timer = 15\n",
        "            else:\n",
        "                self.spawn_timer = 5\n",
        "\n",
        "    def _spawn_car_on_lane(self, lane, y):\n",
        "        is_oncoming = lane < 2\n",
        "        color = ORANGE if is_oncoming else BLUE\n",
        "        speed_range = self.oncoming_speed_range if is_oncoming else self.same_dir_speed_range\n",
        "        speed = self.np_random.uniform(*speed_range)\n",
        "        self.cars.append(Car(lane, y, speed, color))\n",
        "\n",
        "    def _is_lane_safe(self, lane, y_pos):\n",
        "        for car in self.cars:\n",
        "            if car.lane == lane:\n",
        "                if abs(car.y - y_pos) < CAR_HEIGHT * 1.5:\n",
        "                    return False\n",
        "        return True\n",
        "\n",
        "    \n",
        "    def _spawn_initial_traffic(self):\n",
        "        pass\n",
        "\n",
        "    def _spawn_new_car(self):\n",
        "        pass\n",
        "\n",
        "    # make sure cars dont \"overlap\" and instead form traffic \"columns\"\n",
        "    def _enforce_car_spacing(self):\n",
        "        min_dist = CAR_HEIGHT + 10\n",
        "        \n",
        "        for lane in range(4):\n",
        "            lane_cars = [c for c in self.cars if c.lane == lane]\n",
        "            \n",
        "            if len(lane_cars) < 2:\n",
        "                continue\n",
        "            \n",
        "            lane_cars.sort(key=lambda c: c.y)\n",
        "            for i in range(len(lane_cars) - 1):\n",
        "                front_car = lane_cars[i]\n",
        "                back_car = lane_cars[i + 1]\n",
        "                \n",
        "                dist = back_car.y - front_car.y\n",
        "                \n",
        "                if dist < min_dist:\n",
        "                    back_car.y = front_car.y + min_dist\n",
        "    \n",
        "    def _get_lane_from_x(self, x: float) -> int:\n",
        "        for i in range(4):\n",
        "            lane_left = ROAD_LEFT + LANE_WIDTH * i\n",
        "            lane_right = ROAD_LEFT + LANE_WIDTH * (i + 1)\n",
        "            if lane_left <= x < lane_right:\n",
        "                return i\n",
        "        return 3\n",
        "    \n",
        "    def step(self, action: int):\n",
        "        self.steps += 1\n",
        "        \n",
        "        # === DECODIFICARE ACTIUNI COMBINATE (Discrete 9) ===\n",
        "        # 0: Idle\n",
        "        # 1: Accel, 2: Brake, 3: Left, 4: Right\n",
        "        # 5: Accel+Left, 6: Accel+Right, 7: Brake+Left, 8: Brake+Right\n",
        "        \n",
        "        want_accel = action in [1, 5, 6]\n",
        "        want_brake = action in [2, 7, 8]\n",
        "        want_left = action in [3, 5, 7]\n",
        "        want_right = action in [4, 6, 8]\n",
        "\n",
        "\n",
        "        if want_accel:\n",
        "            self.ego_speed = min(self.ego_speed + 2.0, self.max_speed)\n",
        "        elif want_brake:\n",
        "            self.ego_speed = max(self.ego_speed - 3.0, self.min_speed)\n",
        "        else:\n",
        "            self.ego_speed = max(self.ego_speed - 0.5, self.min_speed)  # Coasting friction\n",
        "\n",
        "        if want_left:\n",
        "            self.ego_x = max(self.ego_x - self.move_speed, self.x_min)\n",
        "        elif want_right:\n",
        "            self.ego_x = min(self.ego_x + self.move_speed, self.x_max)\n",
        "        \n",
        "        self.ego_lane = self._get_lane_from_x(self.ego_x)\n",
        "        \n",
        "        self.world_y += self.ego_speed * 0.5\n",
        "        self.total_distance += self.ego_speed\n",
        "        \n",
        "        for car in self.cars:\n",
        "            is_oncoming = car.lane < 2\n",
        "            if is_oncoming:\n",
        "                car.y += (self.ego_speed + car.speed) * 0.15\n",
        "            else:\n",
        "                car.y += (self.ego_speed - car.speed) * 0.15\n",
        "            \n",
        "            # prevent getting REAR-ENDED\n",
        "            if car.lane >= 2:\n",
        "                if car.lane == self.ego_lane:\n",
        "                    if car.y > self.ego_y and (car.y - self.ego_y) < (CAR_HEIGHT + 30):\n",
        "                        # just lower his speed\n",
        "                        if car.speed > self.ego_speed:\n",
        "                            car.speed = self.ego_speed\n",
        "\n",
        "        self._enforce_car_spacing()\n",
        "\n",
        "        self.cars = [c for c in self.cars if c.y < SCREEN_HEIGHT + 100]\n",
        "        self._maintain_traffic_density()\n",
        "        \n",
        "        ego_rect = pygame.Rect(\n",
        "            self.ego_x - CAR_WIDTH // 2,\n",
        "            self.ego_y - CAR_HEIGHT // 2,\n",
        "            CAR_WIDTH,\n",
        "            CAR_HEIGHT\n",
        "        )\n",
        "        \n",
        "        collision = False\n",
        "        for car in self.cars:\n",
        "            if ego_rect.colliderect(car.get_rect()):\n",
        "                collision = True\n",
        "                break\n",
        "        \n",
        "        reward = self._calculate_reward(collision)\n",
        "        \n",
        "        terminated = collision\n",
        "        truncated = self.steps >= 3000  # max steps\n",
        "        \n",
        "        return self._get_obs(), reward, terminated, truncated, {}\n",
        "    \n",
        "    def _calculate_reward(self, collision: bool) -> float:\n",
        "        if collision:\n",
        "            return -10.0\n",
        "        \n",
        "        reward = 0.0\n",
        "        \n",
        "        # speed bonus\n",
        "        reward += (self.ego_speed / self.max_speed) * 0.5\n",
        "        \n",
        "        # crazy, risky driving bonus\n",
        "        if self.ego_x <= LANE_X[1]:\n",
        "            reward += 0.1\n",
        "        \n",
        "        # overtake bonus\n",
        "        for car in self.cars:\n",
        "            if car.lane >= 2:\n",
        "                car_id = id(car)\n",
        "                if car.y > self.ego_y + 20 and car_id not in self.overtaken_cars:\n",
        "                    self.overtaken_cars.add(car_id)\n",
        "                    self.overtakes += 1\n",
        "                    reward += 1.0\n",
        "        \n",
        "        return reward\n",
        "    \n",
        "    def _get_obs(self) -> np.ndarray:\n",
        "        # positia x normalizata (0 = stanga, 1 = dreapta)\n",
        "        x_norm = (self.ego_x - ROAD_LEFT) / (LANE_WIDTH * 4)\n",
        "        \n",
        "        obs = [\n",
        "            x_norm,\n",
        "            self.ego_speed / self.max_speed,\n",
        "            self.ego_lane / 3.0,\n",
        "        ]\n",
        "        \n",
        "        sorted_cars = sorted(self.cars, key=lambda c: abs(c.y - self.ego_y))\n",
        "        \n",
        "        for i in range(10):\n",
        "            if i < len(sorted_cars):\n",
        "                car = sorted_cars[i]\n",
        "                obs.extend([\n",
        "                    (car.x - ROAD_LEFT) / (LANE_WIDTH * 4),  # X normalizat\n",
        "                    (car.y - self.ego_y) / 500.0,  # distanta relativa Y\n",
        "                    car.speed / self.max_speed,\n",
        "                ])\n",
        "            else:\n",
        "                obs.extend([0, 0, 0])  # padding\n",
        "        \n",
        "        return np.array(obs, dtype=np.float32)\n",
        "    \n",
        "    def render(self):\n",
        "        if self.render_mode is None:\n",
        "            return\n",
        "        \n",
        "        if self.screen is None:\n",
        "            pygame.init()\n",
        "            if self.render_mode == \"human\":\n",
        "                pygame.display.set_caption(\"Endless Overtake\")\n",
        "                self.screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
        "            else:\n",
        "                self.screen = pygame.Surface((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
        "            self.clock = pygame.time.Clock()\n",
        "            self.font = pygame.font.Font(None, 36)\n",
        "        \n",
        "        self.screen.fill(GRAY)\n",
        "    \n",
        "        road_rect = pygame.Rect(ROAD_LEFT, 0, LANE_WIDTH * 4, SCREEN_HEIGHT)\n",
        "        pygame.draw.rect(self.screen, BLACK, road_rect)\n",
        "        \n",
        "        # Linii\n",
        "        line_y_offset = int(self.world_y * 2) % 40\n",
        "        \n",
        "        # Margini (continue)\n",
        "        pygame.draw.line(self.screen, WHITE, (ROAD_LEFT, 0), (ROAD_LEFT, SCREEN_HEIGHT), 3)\n",
        "        pygame.draw.line(self.screen, WHITE, (ROAD_LEFT + LANE_WIDTH * 4, 0), \n",
        "                        (ROAD_LEFT + LANE_WIDTH * 4, SCREEN_HEIGHT), 3)\n",
        "        \n",
        "        # Linie centralÄƒ (continuÄƒ galbenÄƒ - separare sensuri)\n",
        "        pygame.draw.line(self.screen, YELLOW, \n",
        "                        (ROAD_LEFT + LANE_WIDTH * 2, 0),\n",
        "                        (ROAD_LEFT + LANE_WIDTH * 2, SCREEN_HEIGHT), 4)\n",
        "        \n",
        "        # Linii punctate Ã®ntre benzi\n",
        "        for lane_sep in [1, 3]:  # Ã®ntre 0-1 È™i 2-3\n",
        "            x = ROAD_LEFT + LANE_WIDTH * lane_sep\n",
        "            for y in range(-40 + line_y_offset, SCREEN_HEIGHT, 40):\n",
        "                pygame.draw.line(self.screen, WHITE, (x, y), (x, y + 20), 2)\n",
        "        \n",
        "        # Deseneaza masinile\n",
        "        for car in self.cars:\n",
        "            is_oncoming = car.lane < 2\n",
        "            self._draw_car(car.x, car.y, car.color, facing_down=is_oncoming)\n",
        "        \n",
        "        # Deseneaza ego (mereu Ã®n sus)\n",
        "        self._draw_car(self.ego_x, self.ego_y, GREEN, facing_down=False)\n",
        "        \n",
        "        # HUD\n",
        "        speed_text = self.font.render(f\"Speed: {self.ego_speed:.0f} km/h\", True, WHITE)\n",
        "        self.screen.blit(speed_text, (10, 10))\n",
        "        \n",
        "        # Speed bar\n",
        "        bar_width = 150\n",
        "        bar_height = 15\n",
        "        bar_x = 10\n",
        "        bar_y = 45\n",
        "        pygame.draw.rect(self.screen, (50, 50, 50), (bar_x, bar_y, bar_width, bar_height))\n",
        "        fill_width = int((self.ego_speed / self.max_speed) * bar_width)\n",
        "        bar_color = GREEN if self.ego_speed < 40 else YELLOW if self.ego_speed < 55 else RED\n",
        "        pygame.draw.rect(self.screen, bar_color, (bar_x, bar_y, fill_width, bar_height))\n",
        "        \n",
        "        overtake_text = self.font.render(f\"Overtakes: {self.overtakes}\", True, WHITE)\n",
        "        self.screen.blit(overtake_text, (10, 70))\n",
        "        \n",
        "        lane_name = [\"CONTRA-L\", \"CONTRA-R\", \"SENS-L\", \"SENS-R\"][self.ego_lane]\n",
        "        on_contra = \"âš ï¸\" if self.ego_x <= LANE_X[1] else \"\"\n",
        "        lane_text = self.font.render(f\"Lane: {lane_name} {on_contra}\", True, WHITE)\n",
        "        self.screen.blit(lane_text, (10, 105))\n",
        "        \n",
        "        if self.render_mode == \"human\":\n",
        "            pygame.display.flip()\n",
        "            self.clock.tick(self.metadata[\"render_fps\"])\n",
        "        \n",
        "        return np.array(pygame.surfarray.array3d(self.screen))\n",
        "    \n",
        "    def _draw_car(self, x: float, y: float, color: Tuple, facing_down: bool = False):\n",
        "        rect = pygame.Rect(\n",
        "            x - CAR_WIDTH // 2,\n",
        "            y - CAR_HEIGHT // 2,\n",
        "            CAR_WIDTH,\n",
        "            CAR_HEIGHT\n",
        "        )\n",
        "        pygame.draw.rect(self.screen, color, rect, border_radius=8)\n",
        "        \n",
        "        # Parbriz (negru) - pozitie diferita pentru contrasens\n",
        "        if facing_down:\n",
        "            # Contrasens: parbrizul jos (spre ego)\n",
        "            windshield = pygame.Rect(\n",
        "                x - CAR_WIDTH // 2 + 6,\n",
        "                y + CAR_HEIGHT // 2 - 28,\n",
        "                CAR_WIDTH - 12,\n",
        "                20\n",
        "            )\n",
        "        else:\n",
        "            # Sens normal: parbrizul sus\n",
        "            windshield = pygame.Rect(\n",
        "                x - CAR_WIDTH // 2 + 6,\n",
        "                y - CAR_HEIGHT // 2 + 8,\n",
        "                CAR_WIDTH - 12,\n",
        "                20\n",
        "            )\n",
        "        pygame.draw.rect(self.screen, BLACK, windshield, border_radius=4)\n",
        "    \n",
        "    def close(self):\n",
        "        if self.screen is not None:\n",
        "            pygame.quit()\n",
        "            self.screen = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0jZo-p1xJ1k"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "class PolicyNet(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super().__init__()\n",
        "        self.policy = nn.Sequential(\n",
        "            nn.Linear(state_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, action_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.policy(x)\n",
        "\n",
        "class ValueNet(nn.Module):\n",
        "    def __init__(self, state_dim):\n",
        "        super().__init__()\n",
        "        self.value = nn.Sequential(\n",
        "            nn.Linear(state_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.value(x)\n",
        "\n",
        "class PPOAgent:\n",
        "    def __init__(self,\n",
        "                 state_dim,\n",
        "                 action_dim,\n",
        "                 gamma=0.99,\n",
        "                 lam=0.95,\n",
        "                 clip_eps=0.2,\n",
        "                 lr=3e-4,\n",
        "                 train_iters=10,\n",
        "                 minibatch_size=64,\n",
        "                 device=\"cpu\"): # <--- Parametru nou\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.lam = lam\n",
        "        self.clip_eps = clip_eps\n",
        "        self.train_iters = train_iters\n",
        "        self.minibatch_size = minibatch_size\n",
        "        self.device = device # <--- SalvÄƒm device-ul\n",
        "\n",
        "        self.policy = PolicyNet(state_dim, action_dim).to(self.device)\n",
        "        self.value_fn = ValueNet(state_dim).to(self.device)\n",
        "\n",
        "        self.opt_policy = optim.Adam(self.policy.parameters(), lr=lr)\n",
        "        self.opt_value = optim.Adam(self.value_fn.parameters(), lr=lr)\n",
        "\n",
        "    def act(self, state):\n",
        "        s = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
        "\n",
        "        logits = self.policy(s)\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        dist = torch.distributions.Categorical(probs)\n",
        "\n",
        "        action = dist.sample()\n",
        "        logp = dist.log_prob(action)\n",
        "        value = self.value_fn(s)\n",
        "\n",
        "        return action.item(), logp.detach(), value.detach()\n",
        "\n",
        "    def get_best_action(self, state):\n",
        "        s = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
        "        logits = self.policy(s)\n",
        "        return torch.argmax(logits).item()\n",
        "\n",
        "    def compute_gae(self, rewards, values, next_value, dones):\n",
        "        values = values + [next_value]\n",
        "        advantages = []\n",
        "        gae = 0\n",
        "\n",
        "        for t in reversed(range(len(rewards))):\n",
        "            delta = rewards[t] + self.gamma * values[t + 1] * (1 - dones[t]) - values[t]\n",
        "            gae = delta + self.gamma * self.lam * (1 - dones[t]) * gae\n",
        "            advantages.insert(0, gae)\n",
        "\n",
        "        returns = [adv + val for adv, val in zip(advantages, values[:-1])]\n",
        "\n",
        "        return torch.tensor(advantages, dtype=torch.float32).to(self.device), \\\n",
        "               torch.tensor(returns, dtype=torch.float32).to(self.device)\n",
        "\n",
        "    def train(self, obs, actions, logp_old, advantages, returns):\n",
        "        dataset_size = len(obs)\n",
        "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "        for _ in range(self.train_iters):\n",
        "            idx = np.random.permutation(dataset_size)\n",
        "\n",
        "            for start in range(0, dataset_size, self.minibatch_size):\n",
        "                batch_idx = idx[start:start + self.minibatch_size]\n",
        "\n",
        "                b_obs = obs[batch_idx]\n",
        "                b_actions = actions[batch_idx]\n",
        "                b_logp_old = logp_old[batch_idx]\n",
        "                b_adv = advantages[batch_idx]\n",
        "                b_returns = returns[batch_idx]\n",
        "\n",
        "                logits = self.policy(b_obs)\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                dist = torch.distributions.Categorical(probs)\n",
        "\n",
        "                new_logp = dist.log_prob(b_actions)\n",
        "                ratio = torch.exp(new_logp - b_logp_old)\n",
        "                clipped = torch.clamp(ratio, 1 - self.clip_eps, 1 + self.clip_eps)\n",
        "\n",
        "                policy_loss = -torch.min(ratio * b_adv, clipped * b_adv).mean()\n",
        "\n",
        "                value_pred = self.value_fn(b_obs).squeeze()\n",
        "                value_loss = (b_returns - value_pred).pow(2).mean()\n",
        "\n",
        "                self.opt_policy.zero_grad()\n",
        "                policy_loss.backward()\n",
        "                self.opt_policy.step()\n",
        "\n",
        "                self.opt_value.zero_grad()\n",
        "                value_loss.backward()\n",
        "                self.opt_value.step()\n",
        "\n",
        "    def save(self, filename):\n",
        "        torch.save(self.policy.state_dict(), filename)\n",
        "\n",
        "    def load(self, filename):\n",
        "        self.policy.load_state_dict(torch.load(filename, map_location=self.device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9lXgHw9xQmw",
        "outputId": "61d52a96-422e-4f3f-db97-65a14eec4b39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸš€ Training on cuda...\n",
            "Epoch 5/300: Avg Reward = 51.94\n",
            "Epoch 10/300: Avg Reward = 51.33\n",
            "Epoch 15/300: Avg Reward = 62.25\n",
            "Epoch 20/300: Avg Reward = 42.35\n",
            "Epoch 25/300: Avg Reward = 53.48\n",
            "Epoch 30/300: Avg Reward = 37.69\n",
            "Epoch 35/300: Avg Reward = 41.47\n",
            "Epoch 40/300: Avg Reward = 39.75\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
        "\n",
        "def train_on_colab():\n",
        "    # run on colllab\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"ðŸš€ Training on {device}...\")\n",
        "\n",
        "    env = EndlessOvertakeEnv(render_mode=None)\n",
        "\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    agent = PPOAgent(state_dim, action_dim, lr=3e-4, device=device)\n",
        "\n",
        "    EPOCHS = 300\n",
        "    STEPS_PER_EPOCH = 2048\n",
        "\n",
        "    reward_history = []\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        observations = []\n",
        "        actions = []\n",
        "        logps = []\n",
        "        values = []\n",
        "        rewards = []\n",
        "        dones = []\n",
        "\n",
        "        state, _ = env.reset()\n",
        "        ep_reward = 0\n",
        "\n",
        "        for step in range(STEPS_PER_EPOCH):\n",
        "            action, logp, value = agent.act(state)\n",
        "\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            observations.append(state)\n",
        "            actions.append(action)\n",
        "            logps.append(logp)\n",
        "            values.append(value)\n",
        "            rewards.append(reward)\n",
        "            dones.append(done)\n",
        "\n",
        "            ep_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                reward_history.append(ep_reward)\n",
        "                state, _ = env.reset()\n",
        "                ep_reward = 0\n",
        "\n",
        "        obs_tensor = torch.tensor(np.array(observations), dtype=torch.float32).to(device)\n",
        "        actions_tensor = torch.tensor(actions).to(device)\n",
        "        logp_tensor = torch.stack(logps).to(device)\n",
        "\n",
        "        values_list = [v.item() for v in values]\n",
        "\n",
        "        next_state_t = torch.tensor(state, dtype=torch.float32).to(device)\n",
        "        next_value = agent.value_fn(next_state_t).item()\n",
        "\n",
        "        advantages, returns = agent.compute_gae(rewards, values_list, next_value, dones)\n",
        "\n",
        "        agent.train(obs_tensor, actions_tensor, logp_tensor, advantages, returns)\n",
        "\n",
        "        avg_rew = np.mean(reward_history[-10:]) if len(reward_history) > 0 else 0\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{EPOCHS}: Avg Reward = {avg_rew:.2f}\")\n",
        "\n",
        "    agent.save(\"traffic_ppo_colab.pth\")\n",
        "\n",
        "    plt.plot(reward_history)\n",
        "    print(\"[!!!]  Model salvat!\")\n",
        "    plt.title(\"Antrenament PPO (Colab)\")\n",
        "    plt.xlabel(\"Episoade\")\n",
        "    plt.ylabel(\"Reward\")\n",
        "    plt.savefig(\"training_plot.png\")\n",
        "    plt.show()\n",
        "\n",
        "train_on_colab()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
